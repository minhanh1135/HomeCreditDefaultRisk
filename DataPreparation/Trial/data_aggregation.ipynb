{"cells":[{"cell_type":"code","execution_count":null,"id":"d0606d99","metadata":{"execution":{"iopub.execute_input":"2023-03-28T10:26:30.886439Z","iopub.status.busy":"2023-03-28T10:26:30.885589Z","iopub.status.idle":"2023-03-28T10:26:30.891355Z","shell.execute_reply":"2023-03-28T10:26:30.889746Z"},"papermill":{"duration":0.015415,"end_time":"2023-03-28T10:26:30.893904","exception":false,"start_time":"2023-03-28T10:26:30.878489","status":"completed"},"tags":[],"id":"d0606d99"},"outputs":[],"source":["import gc "]},{"cell_type":"code","execution_count":null,"id":"742917e1","metadata":{"execution":{"iopub.execute_input":"2023-03-28T10:26:30.909092Z","iopub.status.busy":"2023-03-28T10:26:30.908194Z","iopub.status.idle":"2023-03-28T10:26:31.054962Z","shell.execute_reply":"2023-03-28T10:26:31.053561Z"},"papermill":{"duration":0.16204,"end_time":"2023-03-28T10:26:31.061798","exception":false,"start_time":"2023-03-28T10:26:30.899758","status":"completed"},"tags":[],"id":"742917e1"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import gc \n","from tqdm import tqdm\n","\n","\n","class AGG_DATA():\n","    def __init__(self, path_data):\n","        \"\"\"\n","        path_data: dictionary\n","        \"\"\"\n","        self.path_data = path_data\n","        \n","    \n","    def one_hot_encoder(self, df, nan_as_category = True):\n","        original_columns = list(df.columns)\n","        categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n","        df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n","        new_columns = [c for c in df.columns if c not in original_columns]\n","        return df, new_columns\n","\n","    def grab_col_names(self, dataframe, cat_th=10, car_th=20):\n","        # cat_cols, cat_but_car\n","        cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n","        num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n","                       dataframe[col].dtypes != \"O\"]\n","        cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n","                       dataframe[col].dtypes == \"O\"]\n","        cat_cols = cat_cols + num_but_cat\n","        cat_cols = [col for col in cat_cols if col not in cat_but_car]\n","\n","        # num_cols\n","        num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n","        num_cols = [col for col in num_cols if col not in num_but_cat]\n","\n","        return cat_cols, num_cols, cat_but_car\n","\n","    # Rare Encoding\n","    def rare_encoder(self, dataframe, rare_perc, cat_cols):\n","\n","        rare_columns = [col for col in cat_cols if\n","                        (dataframe[col].value_counts() / len(dataframe) < rare_perc).sum() > 1]\n","\n","        for col in rare_columns:\n","            tmp = dataframe[col].value_counts() / len(dataframe)\n","            rare_labels = tmp[tmp < rare_perc].index\n","            dataframe[col] = np.where(dataframe[col].isin(rare_labels), 'Rare', dataframe[col])\n","\n","        return dataframe\n","    \n","    def iv_woe(self, data, target, bins=10, show_woe=False):\n","\n","        #Empty Dataframe\n","        newDF,woeDF = pd.DataFrame(), pd.DataFrame()\n","\n","        #Extract Column Names\n","        cols = data.columns\n","\n","        #Run WOE and IV on all the independent variables\n","        for ivars in cols[~cols.isin([target])]:\n","    #     for ivars in ['AMT_REQ_CREDIT_BUREAU_YEAR']:\n","            if (data[ivars].dtype.kind in 'bifc') and (len(np.unique(data[ivars]))>10):\n","                binned_x = pd.qcut(data[ivars], bins,  duplicates='drop')\n","                d0 = pd.DataFrame({'x': binned_x, 'y': data[target]})\n","            else:\n","                d0 = pd.DataFrame({'x': data[ivars], 'y': data[target]})\n","\n","            # missing\n","            if d0.isnull().sum().sum() > 0 :\n","                try:\n","                    d0['x'] = d0['x'].cat.add_categories('Missing')\n","                    d0['x'].fillna('Missing', inplace =True)\n","                except:\n","    #                 print('var ', ivars)\n","                    d0['x'].fillna('Missing', inplace =True)\n","\n","\n","            # Calculate the number of events in each group (bin)\n","            d = d0.groupby(\"x\", as_index=False).agg({\"y\": [\"count\", \"sum\"]})\n","            d.columns = ['Cutoff', 'N', 'Events']\n","\n","            # Calculate % of events in each group.\n","            d['% of Events'] = np.maximum(d['Events'], 0.5) / d['Events'].sum()\n","\n","            # Calculate the non events in each group.\n","            d['Non-Events'] = d['N'] - d['Events']\n","            # Calculate % of non events in each group.\n","            d['% of Non-Events'] = np.maximum(d['Non-Events'], 0.5) / d['Non-Events'].sum()\n","\n","            # Calculate WOE by taking natural log of division of % of non-events and % of events\n","            d['WoE'] = np.log(d['% of Events']/d['% of Non-Events'])\n","            d['IV'] = d['WoE'] * (d['% of Events'] - d['% of Non-Events'])\n","            d.insert(loc=0, column='Variable', value=ivars)\n","    #         print(\"Information value of \" + ivars + \" is \" + str(round(d['IV'].sum(),6)))\n","            temp =pd.DataFrame({\"Variable\" : [ivars], \"IV\" : [d['IV'].sum()]}, columns = [\"Variable\", \"IV\"])\n","            newDF=pd.concat([newDF,temp], axis=0)\n","            woeDF=pd.concat([woeDF,d], axis=0)\n","\n","            #Show WOE Table\n","            if show_woe == True:\n","                print(d)\n","        return newDF, woeDF\n","    \n","    ## agg: install \n","    def installments_payments(self, nan_as_category = True):\n","        df = pd.read_csv(self.path_data['installments_payments'])\n","        cat_cols, num_cols, cat_but_car = self.grab_col_names(df)\n","        df, cat_cols = self.one_hot_encoder(df, nan_as_category= True) ### neden cat var?\n","\n","\n","\n","        # The difference between the amount paid in each loan installment payment and the original and its percentage\n","        df['PAYMENT_PERC'] = df['AMT_PAYMENT'] / df['AMT_INSTALMENT']\n","        df['PAYMENT_DIFF'] = df['AMT_INSTALMENT'] - df['AMT_PAYMENT']\n","\n","       # Days overdue and days before due -- only positive values are taken\n","        df['DPD'] = df['DAYS_ENTRY_PAYMENT'] - df['DAYS_INSTALMENT']\n","        df['DBD'] = df['DAYS_INSTALMENT'] - df['DAYS_ENTRY_PAYMENT']\n","        df['DPD'] = df['DPD'].apply(lambda x: x if x > 0 else 0)\n","        df['DBD'] = df['DBD'].apply(lambda x: x if x > 0 else 0)\n","\n","        # Whether each installment payment is late or not 1: late paid 0: represents early payment\n","        df['NEW_DAYS_PAID_EARLIER'] = df['DAYS_INSTALMENT'] - df['DAYS_ENTRY_PAYMENT']\n","        df['NEW_NUM_PAID_LATER'] = df['NEW_DAYS_PAID_EARLIER'].map(lambda x: 1 if x<0 else 0)\n","\n","        # Numeric Features\n","        aggregations = {\n","            'NUM_INSTALMENT_VERSION': ['nunique'],\n","            'DPD': ['max', 'mean', 'sum'],\n","            'DBD': ['max', 'mean', 'sum'],\n","            'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n","            'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n","            'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n","            'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n","            'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n","        }\n","\n","        # Categorical Features\n","        for cat in cat_cols:\n","            aggregations[cat] = ['mean']\n","        ins_agg = df.groupby('SK_ID_CURR').agg(aggregations)\n","        ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n","        # Count installments accounts\n","        ins_agg['INSTAL_COUNT'] = df.groupby('SK_ID_CURR').size()\n","        del df\n","        gc.collect()\n","        #print(ins_agg.columns.tolist())\n","        return ins_agg\n","    \n","    # application\n","    def previous_application(self, nan_as_category = True):\n","        df = pd.read_csv(self.path_data['previous_application'])\n","        cat_cols, num_cols, cat_but_car = self.grab_col_names(df)\n","\n","        df['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n","        df['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n","        df['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n","        df['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n","        df['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n","\n","\n","\n","        na = ['XNA', 'XAP']\n","        for col in cat_cols:\n","            for n in na:\n","                df.loc[df[col] == n, col] = np.nan\n","\n","        self.rare_encoder(df, 0.01,cat_cols)\n","\n","\n","\n","        a = ['Auto Accessories', 'Jewelry', 'Homewares', 'Medical Supplies', 'Vehicles', 'Sport and Leisure', \n","             'Gardening', 'Other', 'Office Appliances', 'Tourism', 'Medicine', 'Direct Sales', 'Fitness', 'Additional Service', \n","             'Education', 'Weapon', 'Insurance', 'House Construction', 'Animals'] \n","\n","        df[\"NAME_GOODS_CATEGORY\"] = df[\"NAME_GOODS_CATEGORY\"].replace(a, 'others')\n","\n","        # \"NAME_SELLER_INDUSTRY\n","        df[\"NAME_SELLER_INDUSTRY\"] = df[\"NAME_SELLER_INDUSTRY\"].replace(\"Rare\", 'others')\n","\n","        # PREV_NAME_GOODS_CATEGORY\n","        df[\"NAME_GOODS_CATEGORY\"] = np.where(df[\"NAME_GOODS_CATEGORY\"].str.contains(\"Photo / Cinema Equipment\"),\n","                                           \"Photo_Cinema_Equipment\", df[\"NAME_GOODS_CATEGORY\"])\n","\n","        # CHANNEL_TYPE\n","        df[\"CHANNEL_TYPE\"] = np.where(df[\"CHANNEL_TYPE\"].str.contains(\"Regional / Local\"),\n","                                           \"Regional_Local\", df[\"CHANNEL_TYPE\"])\n","\n","        df[\"NAME_GOODS_CATEGORY\"] = np.where(df[\"NAME_GOODS_CATEGORY\"].str.contains(\"Audio/Video\"),\n","                                           \"Audio_Video\", df[\"NAME_GOODS_CATEGORY\"])\n","\n","\n","\n","\n","        del_cols = ['RATE_INTEREST_PRIMARY', 'RATE_INTEREST_PRIVILEGED', 'DAYS_FIRST_DRAWING',\n","                    'NAME_CASH_LOAN_PURPOSE', 'CODE_REJECT_REASON', 'FLAG_LAST_APPL_PER_CONTRACT',\n","                    'NFLAG_LAST_APPL_IN_DAY', 'SELLERPLACE_AREA']\n","\n","        df.drop(del_cols, axis=1, inplace=True)\n","\n","\n","\n","        df[\"NEW_HOUR_APPR_PROCESS_START_CAT\"]= df.loc[:,\"HOUR_APPR_PROCESS_START\"]\n","        a = [8,9,10,11,12,13,14,15,16,17]\n","        df[\"NEW_HOUR_APPR_PROCESS_START_CAT\"] = df[\"NEW_HOUR_APPR_PROCESS_START_CAT\"].replace(a, \"WORK_HOURS\")\n","        b = [18,19,20,21,22,23,0,1,2,3,4,5,6,7]\n","        df[\"NEW_HOUR_APPR_PROCESS_START_CAT\"] = df[\"NEW_HOUR_APPR_PROCESS_START_CAT\"].replace(b, 'OFF_HOURS')\n","\n","        # 2. \"WEEKDAY_APPR_PROCESS_START\" We have reduced the variable to two classes as WEEK_DAY and WEEKEND\n","        df[\"NEW_WEEKDAY_APPR_PROCESS_START_CAT\"] = df.loc[:,\"WEEKDAY_APPR_PROCESS_START\"]\n","        df[\"NEW_WEEKDAY_APPR_PROCESS_START_CAT\"] = df[\"NEW_WEEKDAY_APPR_PROCESS_START_CAT\"].replace(['MONDAY','TUESDAY', 'WEDNESDAY','THURSDAY','FRIDAY'], 'WEEK_DAY')\n","        df[\"NEW_WEEKDAY_APPR_PROCESS_START_CAT\"] = df[\"NEW_WEEKDAY_APPR_PROCESS_START_CAT\"].replace(['SATURDAY', 'SUNDAY'], 'WEEKEND')\n","\n","        # 3. \"NAME_TYPE_SUITE\"  dividing the variable into two categories as single and multiple\n","        df[\"NAME_TYPE_SUITE\"] = df[\"NAME_TYPE_SUITE\"].replace('Unaccompanied', 'single')\n","        b = ['Family', 'Spouse, partner', 'Children', 'Other_B', 'Other_A', 'Group of people']\n","        df[\"NAME_TYPE_SUITE\"] = df[\"NAME_TYPE_SUITE\"].replace(b, 'multiple')\n","\n","        # 4. How much credit did the client request in the previous application / final credit amount of the previous application\n","        df[\"NEW_AMT_CREDIT_RATIO\"] = df[\"AMT_APPLICATION\"]/df[\"AMT_CREDIT\"]\n","\n","        # Having less than the desired credit, which should be close to 1\n","        # Having more than the desired credit, which should be close to 0\n","        # If 1, the loan amount requested and the loan amount must be the same.\n","\n","        # 5. If x <= 1, he got the loan he wanted or more.\n","        df[\"NEWX2_FLAG_AMT_CREDIT_RATIO\"] = df[\"NEW_AMT_CREDIT_RATIO\"].apply(lambda x: 1 if(x<=1) else 0)\n","\n","\n","\n","        df[(df['AMT_CREDIT'] == 0) | (df['AMT_GOODS_PRICE'] == 0)]['NEW_INSURED_ON_APPROVAL'] = np.nan\n","        df['INSURANCE_AMOUNT'] = df['AMT_CREDIT'] - df['AMT_GOODS_PRICE']\n","        df['NEW_INSURED_ON_APPROVAL'] = df['INSURANCE_AMOUNT'].apply(lambda x: 1 if x > 0 else (0 if x <= 0 else np.nan))\n","        df.drop('INSURANCE_AMOUNT', axis=1, inplace=True)\n","\n","        # 7. How many years did he pay = Amount of loan / annual installment given by the bank\n","        df['NEW_HOW_PAID_YEARS'] = df['AMT_CREDIT'] / df['AMT_ANNUITY']\n","\n","       # 8. The amount of loan he applied for / the price of the product he wants to buy\n","        df['NEW_GOODS_RATIO'] = df['AMT_APPLICATION'] / df['AMT_GOODS_PRICE']\n","\n","\n","        df['NEW_LATE_DAYS'] =  df['DAYS_LAST_DUE_1ST_VERSION'] - df['DAYS_FIRST_DUE'] \n","\n","        # 10 .CLASSIFIED ONE\n","        k = df[\"DAYS_LAST_DUE_1ST_VERSION\"] - df[\"DAYS_LAST_DUE\"]\n","        df[\"NEW_FLAG_LATE_DAYS\"] = [1 if i >= 0 else (0 if i < 0  else \"NaN\") for i in k]\n","\n","       # WEEKDAY_APPR_PROCESS_START_DIC\n","       # Cycle encoding can be used in cyclic variables such as day, month, year.\n","        df['WEEKDAY_APPR_PROCESS_START_DIC'] = df['WEEKDAY_APPR_PROCESS_START'].map({\n","            'MONDAY': 1, 'TUESDAY': 2, 'WEDNESDAY': 3, 'THURSDAY': 4, 'FRIDAY': 5, 'SATURDAY': 6, 'SUNDAY': 7})\n","        df['NEW_WEEKDAY_SIN'] = np.sin(2 * np.pi * df['WEEKDAY_APPR_PROCESS_START_DIC'] / 7)\n","        df['NEW_WEEKDAY_COS'] = np.cos(2 * np.pi * df['WEEKDAY_APPR_PROCESS_START_DIC'] / 7)\n","\n","\n","\n","        df, cat_cols = self.one_hot_encoder(df, nan_as_category= True)\n","\n","\n","        col_list = df.columns.tolist()\n","        id_list = [\"SK_ID_CURR\",\"SK_ID_PREV\"]\n","        num_list = [col for col in col_list if col not in cat_cols + id_list]\n","\n","        # Previous applications numeric features\n","        agg_num_prev = {}\n","        for num in num_list:\n","            agg_num_prev[num] = ['min', 'max', 'mean', 'median']\n","\n","        # Previous applications categorical features\n","        agg_cat_prev = {}\n","        for cat in cat_cols:\n","            agg_cat_prev[cat] = ['mean']\n","\n","        prev_agg = df.groupby('SK_ID_CURR').agg({**agg_num_prev, **agg_cat_prev})\n","        prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n","\n","        # Previous Applications: Approved Applications - only numerical features\n","        approved = df[df['NAME_CONTRACT_STATUS_Approved'] == 1]\n","        approved_agg = approved.groupby('SK_ID_CURR').agg(agg_num_prev)\n","        approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n","        prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n","\n","        # Previous Applications: Refused Applications - only numerical features\n","        refused = df[df['NAME_CONTRACT_STATUS_Refused'] == 1]\n","        refused_agg = refused.groupby('SK_ID_CURR').agg(agg_num_prev)\n","        refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n","        prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n","\n","        del refused, refused_agg, approved, approved_agg, df\n","        gc.collect()\n","        #print(prev_agg.columns.tolist())\n","        return prev_agg\n","    \n","    \n","    def credit_card_balance(self, nan_as_category = True):\n","        df = pd.read_csv(self.path_data['credit_card_balance'])\n","        cat_cols, num_cols, cat_but_car = self.grab_col_names(df)\n","\n","        # Rare\n","        df[\"NAME_CONTRACT_STATUS\"] = np.where(~(df[\"NAME_CONTRACT_STATUS\"].isin([\"Active\", \"Completed\"])),\n","                                              \"Rare\", df[\"NAME_CONTRACT_STATUS\"])\n","\n","        # One Hot Encoder\n","        df, cat_cols = self.one_hot_encoder(df, nan_as_category=False)\n","\n","\n","        # Amount withdrawn from ATM + amount of goods purchase\n","        df[\"TOTAL_SPENDING\"] = df[\"AMT_DRAWINGS_ATM_CURRENT\"] + df[\"AMT_DRAWINGS_POS_CURRENT\"]\n","\n","        # The amount paid by the customer during the month - the minimum monthly installment\n","        df[\"REGULARITY_PAYMENT\"] = df[\"AMT_INST_MIN_REGULARITY\"] - df[\"AMT_PAYMENT_TOTAL_CURRENT\"]\n","\n","        # General aggregations\n","        df.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n","        cc_agg = df.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n","        cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n","        # Count credit card lines\n","        cc_agg['CC_COUNT'] = df.groupby('SK_ID_CURR').size()\n","        del df\n","        gc.collect()\n","        return cc_agg\n","    \n","    def agg_pos_cash_time(self, time, nan_as_category = True):\n","        df = pd.read_csv(self.path_data['POS_CASH_balance'])\n","        if time != 'full':\n","            df = df[df['MONTHS_BALANCE'] >= -time]\n","        cat_cols, num_cols, cat_but_car = self.grab_col_names(df)\n","\n","        # Rare\n","        self.rare_encoder(df, 0.01, cat_cols)\n","        # One-Hot Encoding\n","        df, cat_cols = self.one_hot_encoder(df, nan_as_category= True)\n","\n","        # Numerical Features\n","        aggregations = {'MONTHS_BALANCE': ['max', 'mean', 'size'],\n","                        'CNT_INSTALMENT': ['max', 'mean', 'std', 'min', 'median'],\n","                        'CNT_INSTALMENT_FUTURE': ['max', 'mean', 'sum', 'min', 'median', 'std'],\n","                        'SK_DPD': ['max', 'mean', 'sum'],\n","                        'SK_DPD_DEF': ['max', 'mean', 'sum']\n","                       }\n","    \n","        # Categorical Features\n","        original_columns = list(df.columns)\n","        new_columns = [c for c in df.columns if c not in original_columns]\n","        for cat in new_columns:\n","            aggregations[cat] = ['mean']\n","\n","        pos_agg = df.groupby('SK_ID_CURR').agg(aggregations)\n","        pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() + '_C{}'.format(time) for e in pos_agg.columns.tolist()])\n","        # Count pos cash accounts\n","        pos_agg['POS_COUNT'] = df.groupby('SK_ID_CURR').size()\n","        del df\n","        gc.collect()\n","        #print(pos_agg.columns.tolist())\n","        return pos_agg\n","    \n","    \n","    def agg_bureau_and_balance(self, nan_as_category = True):\n","        df = pd.read_csv(self.path_data['bureau_balance'])\n","        bureau = pd.read_csv(self.path_data['bureau'])\n","\n","        df,df_cat=self.one_hot_encoder(df)\n","\n","        # Bureau balance: Perform aggregations and merge with bureau.csv\n","        agg_list = {'MONTHS_BALANCE': ['min', 'max', 'size'] }\n","        for col in df_cat:\n","            agg_list[col] = ['mean','sum']\n","\n","        bb_agg = df.groupby(\"SK_ID_BUREAU\").agg(agg_list)\n","        # Renaming variable names\n","        bb_agg.columns = pd.Index([col[0] + \"_\" + col[1].upper() for col in bb_agg.columns.tolist()])\n","        # New feature\n","        bb_agg['NEW_STATUS_SCORE'] = bb_agg['STATUS_1_SUM'] + bb_agg['STATUS_2_SUM']^2 + bb_agg['STATUS_3_SUM']^3 + bb_agg['STATUS_4_SUM']^4 + bb_agg['STATUS_5_SUM']^5\n","\n","        bureau_and_bb = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n","        bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n","        del df, bb_agg\n","        gc.collect()\n","\n","        cat_cols, num_cols, cat_but_car = self.grab_col_names(bureau_and_bb)\n","        self.rare_encoder(bureau_and_bb,0.2,cat_cols)\n","\n","        #CREDIT_ACTIVE reduced the number of classes of variable to 2 (Closed-Active)\n","        bureau_and_bb['CREDIT_ACTIVE'] = bureau_and_bb['CREDIT_ACTIVE'].replace(\"Rare\", 'Active')\n","        #CREDIT_CURRENCY The 1 class of the variable carries meaningless information because it covers 99% of the data set, that is, it is unbalanced data. Therefore, we will remove it.\n","        bureau_and_bb.drop(\"CREDIT_CURRENCY\", inplace = True, axis = 1)\n","\n","\n","        bureau_and_bb.loc[(bureau_and_bb['CREDIT_ACTIVE'] == 'Active') & (bureau_and_bb['DAYS_CREDIT_ENDDATE'] < 0), 'NEW_EARLY_ACTİVE'] = 1\n","        bureau_and_bb.loc[(bureau_and_bb['CREDIT_ACTIVE'] == 'Closed') & (abs(bureau_and_bb['DAYS_CREDIT_ENDDATE']) < abs(bureau_and_bb['DAYS_ENDDATE_FACT']) ), 'NEW_EARLY_CLOSED'] = 1\n","\n","        # 2. Replacing Extended Credits by 1\n","        bureau_and_bb[\"NEW_CNT_CREDIT_PROLONG_CAT\"] = bureau_and_bb.loc[:,'CNT_CREDIT_PROLONG']\n","        prolong = [1,2,3,4,5,6,7,8,9]\n","        bureau_and_bb[\"NEW_CNT_CREDIT_PROLONG_CAT\"] = bureau_and_bb['NEW_CNT_CREDIT_PROLONG_CAT'].replace(prolong, 1)\n","\n","        # 3. How many different loan types did the person receive?\n","        temp_bu = bureau_and_bb[['SK_ID_CURR', 'CREDIT_TYPE']].groupby(by=['SK_ID_CURR'])['CREDIT_TYPE'].nunique().reset_index().rename(index=str, columns={'CREDIT_TYPE': 'NEW_BUREAU_LOAN_TYPES'})\n","        bureau_and_bb = bureau_and_bb.merge(temp_bu, on=['SK_ID_CURR'], how='left')\n","\n","        # 4. Debt Ratio\n","        # Current loan to Credit Bureau / current loan to the credit bureau\n","        # The reason for adding by 1 is not to be undefined.\n","        bureau_and_bb['NEW_DEPT_RATIO'] = bureau_and_bb['AMT_CREDIT_SUM_DEBT'] / (bureau_and_bb['AMT_CREDIT_SUM']+1)\n","\n","        # 5.Is the credit update new?\n","        # We are based on 90 days. Because the transaction is started after 3 months delay in banks.\n","        bureau_and_bb['NEWS_DAYS_CREDIT_UPDATE'] = bureau_and_bb['DAYS_CREDIT_UPDATE'].apply(lambda x : 'old' if x < -90 else 'new')\n","\n","\n","        bureau_and_bb, bureau_and_bb_cat = self.one_hot_encoder(bureau_and_bb)\n","\n","        # Bureau and bureau_balance numeric features\n","        num_aggregations = {'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n","                            'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n","                            'DAYS_CREDIT_UPDATE': ['mean'],\n","                            'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n","                            'DAYS_ENDDATE_FACT':['min', 'max', 'mean'],\n","                            'NEW_STATUS_SCORE':['min','mean','max'],\n","                            'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n","                            'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n","                            'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n","                            'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n","                            'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n","                            'AMT_ANNUITY': ['max', 'mean'],\n","                            'CNT_CREDIT_PROLONG': ['sum'],\n","                            'MONTHS_BALANCE_MIN': ['min'],\n","                            'MONTHS_BALANCE_MAX': ['max'],\n","                            'MONTHS_BALANCE_SIZE': ['mean', 'sum'],\n","                            'NEW_DEPT_RATIO': ['min','max','mean'] }\n","\n","        # Bureau and bureau_balance categorical features\n","        cat_aggregations = {}\n","        for cat in bureau_and_bb_cat: cat_aggregations[cat] = ['mean']\n","        for cat in df_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n","\n","        bureau_agg = bureau_and_bb.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n","        bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n","\n","        # Bureau: Active credits - using only numerical aggregations\n","        active = bureau_and_bb[bureau_and_bb['CREDIT_ACTIVE_Active'] == 1]\n","        active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n","        active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n","        bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n","        del active, active_agg\n","\n","        # Bureau: Closed credits - using only numerical aggregations\n","        closed = bureau_and_bb[bureau_and_bb['CREDIT_ACTIVE_Closed'] == 1]\n","        closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n","        closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n","        bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n","        del closed, closed_agg, bureau\n","        gc.collect()\n","        #print(bureau_agg.columns.tolist())\n","        return bureau_agg\n","    \n","    \n","    def agg_application_train(self, nan_as_category = False):\n","    \n","        # Reading the Dataset\n","        df = pd.read_csv(self.path_data['application_train'])\n","#         test_df = pd.read_csv('../input/home-credit-default-risk/application_test.csv', nrows= num_rows)\n","#         print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n","#         df = df.append(test_df).reset_index()\n","\n","        # There are 4 people whose gender is not specified, we remove them.\n","        df = df[df['CODE_GENDER'] != 'XNA']\n","\n","        # There is 1 person whose marital status is unknown, we dropped it.\n","        df = df[df['NAME_FAMILY_STATUS'] != \"Unknown\" ]\n","\n","        # NaN values for DAYS_EMPLOYED: 365243 -> nan\n","        df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n","\n","\n","\n","        df.loc[df['NAME_INCOME_TYPE'] == 'Businessman', 'NAME_INCOME_TYPE'] = 'Commercial associate'\n","        df.loc[df['NAME_INCOME_TYPE'] == 'Maternity leave', 'NAME_INCOME_TYPE'] = 'Pensioner'\n","        df.loc[df['NAME_INCOME_TYPE'] == 'Student', 'NAME_INCOME_TYPE'] = 'State servant'\n","        df.loc[df['NAME_INCOME_TYPE'] == 'Unemployed', 'NAME_INCOME_TYPE'] = 'Pensioner'\n","\n","        # ORGANIZATION_TYPE\n","        df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].str.contains(\"Business Entity\"), \n","                                           \"Business_Entity\", df[\"ORGANIZATION_TYPE\"])\n","        df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].str.contains(\"Industry\"), \n","                                           \"Industry\", df[\"ORGANIZATION_TYPE\"])\n","        df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].str.contains(\"Trade\"),\n","                                           \"Trade\", df[\"ORGANIZATION_TYPE\"])\n","        df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].str.contains(\"Transport\"),\n","                                           \"Transport\", df[\"ORGANIZATION_TYPE\"])\n","        df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"School\", \"Kindergarten\", \"University\"]),\n","                                           \"Education\", df[\"ORGANIZATION_TYPE\"])\n","        df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"Emergency\",\"Police\", \"Medicine\",\"Goverment\", \"Postal\", \"Military\", \"Security Ministries\", \"Legal Services\"]), \"Official\", df[\"ORGANIZATION_TYPE\"])\n","        df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"Bank\", \"Insurance\"]),\n","                                           \"Finance\", df[\"ORGANIZATION_TYPE\"])\n","        df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].str.contains(\"Goverment\"), \n","                                           \"Government\", df[\"ORGANIZATION_TYPE\"])\n","        df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"Realtor\", \"Housing\"]), \"Realty\", df[\"ORGANIZATION_TYPE\"])\n","        df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"Hotel\", \"Restaurant\",\"Services\"]), \"TourismFoodSector\", df[\"ORGANIZATION_TYPE\"])\n","        df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"Cleaning\",\"Electricity\", \"Telecom\", \"Mobile\", \"Advertising\", \"Religion\", \"Culture\"]), \"Other\", df[\"ORGANIZATION_TYPE\"])\n","\n","\n","        # OCCUPATION_TYPE\n","        df[\"OCCUPATION_TYPE\"] = np.where(df[\"OCCUPATION_TYPE\"].isin([\"Low-skill Laborers\", \"Cooking staff\", \"Security staff\", \"Private service staff\", \"Cleaning staff\", \"Waiters/barmen staff\"]), \"Low_skill_staff\", df[\"OCCUPATION_TYPE\"])\n","        df[\"OCCUPATION_TYPE\"] = np.where(df[\"OCCUPATION_TYPE\"].isin([\"IT staff\", \"High skill tech staff\"]), \"High_skill_staff\", df[\"OCCUPATION_TYPE\"])\n","        df[\"OCCUPATION_TYPE\"] = np.where(df[\"OCCUPATION_TYPE\"].isin([\"Secretaries\", \"HR staff\",\"Realty agents\"]), \"Others\", df[\"OCCUPATION_TYPE\"])\n","\n","        # NAME_TYPE_SUITE\n","        rare_list = [\"NAME_TYPE_SUITE\"]\n","        self.rare_encoder(df, 0.01, rare_list)\n","\n","\n","\n","        df[\"NAME_EDUCATION_TYPE\"] = np.where(df[\"NAME_EDUCATION_TYPE\"] == \"Academic degree\",\n","                                             \"Higher education\", df[\"NAME_EDUCATION_TYPE\"])\n","\n","        df[\"NAME_EDUCATION_TYPE\"] = np.where(df[\"NAME_EDUCATION_TYPE\"].str.contains(\"Secondary / secondary special\"),\n","                                             \"Secondary_secondary_special\", df[\"ORGANIZATION_TYPE\"])\n","\n","        # NAME_FAMILY_STATUS\n","        df[\"NAME_FAMILY_STATUS\"] = np.where(df[\"NAME_FAMILY_STATUS\"].str.contains(\"Single / not married\"),\n","                                            \"Single_not_married\", df[\"NAME_FAMILY_STATUS\"])\n","\n","\n","        # NAME_HOUSING_TYPE\n","        df[\"NAME_HOUSING_TYPE\"] = np.where(df[\"NAME_HOUSING_TYPE\"].str.contains(\"House / apartment\"),\n","                                           \"House_apartment\", df[\"NAME_HOUSING_TYPE\"])\n","\n","        # NAME_TYPE_SUITE\n","        df[\"NAME_TYPE_SUITE\"] = np.where(df[\"NAME_TYPE_SUITE\"].str.contains(\"Spouse, partner\"),\n","                                           \"Spouse_partner\", df[\"NAME_TYPE_SUITE\"])\n","\n","\n","        # NAME_CONTRACT_TYPE\n","        # We have binary encoded variables that are categorical but will be encoded as 0 and 1, such as gender.\n","        for bin_feature in [\"NAME_CONTRACT_TYPE\", 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n","            df[bin_feature], uniques = pd.factorize(df[bin_feature])\n","\n","\n","\n","        weekday_dict = {'MONDAY': 1, 'TUESDAY': 2, 'WEDNESDAY': 3, 'THURSDAY': 4, 'FRIDAY': 5, 'SATURDAY': 6, 'SUNDAY': 7}\n","        df.replace({'WEEKDAY_APPR_PROCESS_START': weekday_dict}, inplace=True)\n","        # Cycle encode\n","        df['NEW_WEEKDAY_APPR_PROCESS_START' + \"_SIN\"] = np.sin(2 * np.pi * df[\"WEEKDAY_APPR_PROCESS_START\"]/7)\n","        df[\"NEW_WEEKDAY_APPR_PROCESS_START\" + \"_COS\"] = np.cos(2 * np.pi * df[\"WEEKDAY_APPR_PROCESS_START\"]/7)\n","\n","\n","\n","        df['NEW_HOUR_APPR_PROCESS_START' + \"_SIN\"] = np.sin(2 * np.pi * df[\"HOUR_APPR_PROCESS_START\"]/23)\n","        df[\"NEW_HOUR_APPR_PROCESS_START\" + \"_COS\"] = np.cos(2 * np.pi * df[\"HOUR_APPR_PROCESS_START\"]/23)\n","\n","\n","\n","        drop_cols = [\"FONDKAPREMONT_MODE\", \"WALLSMATERIAL_MODE\", \"HOUSETYPE_MODE\",\n","                     \"EMERGENCYSTATE_MODE\",\"FLAG_MOBIL\", \"FLAG_EMP_PHONE\",\"FLAG_WORK_PHONE\", \"FLAG_CONT_MOBILE\", \"FLAG_PHONE\", \"FLAG_EMAIL\"]\n","        df.drop(drop_cols, axis = 1, inplace = True)\n","\n","        df.drop(['OBS_30_CNT_SOCIAL_CIRCLE','OBS_60_CNT_SOCIAL_CIRCLE'], axis = 1, inplace = True)\n","\n","\n","\n","        cols = [\"REG_REGION_NOT_LIVE_REGION\",\"REG_REGION_NOT_WORK_REGION\", \"LIVE_REGION_NOT_WORK_REGION\", \n","                \"REG_CITY_NOT_LIVE_CITY\",\"REG_CITY_NOT_WORK_CITY\",\"LIVE_CITY_NOT_WORK_CITY\"]\n","        df[\"NEW_REGION\"] = df[cols].sum(axis = 1)\n","        df.drop(cols, axis = 1, inplace = True)\n","\n","\n","        docs = [col for col in df.columns if 'FLAG_DOC' in col]\n","        df['NEW_DOCUMENT'] = df[docs].sum(axis=1)\n","        df.drop(docs, axis = 1, inplace = True)\n","\n","\n","\n","        # 1. How long ago did the client start working (days) / client's age (days)\n","        df['NEW_DAYS_EMPLOYED_RATIO'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n","\n","        # 2. Customer's total income / loan amount\n","        df['NEW_INCOME_CREDIT_RATIO'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n","\n","        # 3. Customer's total income / family member time\n","        # It reveals how much income per person in the family has.\n","        df['NEW_INCOME_PER_RATIO'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n","\n","        # 4. Annual payment of the loan / Income of the customer\n","        # If this value is between 0-1, it is good, that is, the income is more than the loan payment.\n","        # If this value is 1, it is bad, that is, his income is less than the loan payment.\n","        df['NEW_ANNUITY_INCOME_RATIO'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n","\n","        # 5. Annual payment of the loan / loan amount\n","        df['NEW_PAYMENT_RATIO'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n","\n","        # 6. EXT_SOURCE variables were the scores taken from the outside. We created a new variable with their average.\n","        df[\"NEW_EXTSOURCE_MEAN\"] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n","\n","        # 7. We created a new weighted variable by multiplying these variables.\n","        df['NEW_EXTSOURCES_WPOINT'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n","\n","        # 8. Price of goods to be purchased with credit / total credit amount\n","        # If this ratio is between 0-1, it means that the customer takes more credit than the price of the goods they will buy.\n","        # If this ratio is 1, it means that the customer receives as much credit as he needs.\n","        # If this ratio is greater than 1, it means that the customer has taken less credit than he needs.\n","        df[\"NEW_GOODS_CREDIT_RATIO\"] = df[\"AMT_GOODS_PRICE\"] / df[\"AMT_CREDIT\"]\n","\n","        # 9. In connection with the above variant:\n","        # If this difference is greater than 0, he has taken less credit than he needs.\n","        # If it is 0, he has taken as much as he needs.\n","        # If it is less than 0, it has taken more than it needs.\n","        df[\"NEW_GOODS_CREDIT_DIFF\"] = df[\"AMT_GOODS_PRICE\"] - df[\"AMT_CREDIT\"]\n","\n","        # 10. (Price of goods to be purchased on credit / total credit amount) / total income\n","        df[\"NEW_GOODS_CREDIT_DIFF_RATIO\"] = (df[\"AMT_GOODS_PRICE\"] - df[\"AMT_CREDIT\"]) / df[\"AMT_INCOME_TOTAL\"]\n","\n","        # 11. Total revenue / age of customer (in days)\n","        df['NEW_INCOME_BIRTH_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_BIRTH']\n","\n","        # 12. The DAYS BITH variable gave the person's age in days.\n","        # But the values ​​- (because it gives the information that he was born so many days ago.)\n","        # So to find the age of the customer, we will multiply by - and divide by 360.\n","        df[\"NEW_DAYS_BIRTH\"] = round(df[\"DAYS_BIRTH\"]* -1 / 365)\n","\n","        # 13. Segmenting customers by age\n","        df.loc[df[\"NEW_DAYS_BIRTH\"] <= 34 ,\"NEW_SEGMENT_AGE\"] = \"Young\"\n","        df.loc[(df[\"NEW_DAYS_BIRTH\"] > 34)&(df[\"NEW_DAYS_BIRTH\"] <= 54) ,\"NEW_SEGMENT_AGE\"] = \"Middle_Age\"\n","        df.loc[(df[\"NEW_DAYS_BIRTH\"] > 54),\"NEW_SEGMENT_AGE\"] = \"Old\"\n","\n","        # 14. Segmenting customers by revenue\n","        df.loc[df[\"AMT_INCOME_TOTAL\"] <= 112500 ,\"NEW_SEGMENT_INCOME\"] = \"Low_Income\"\n","        df.loc[(df[\"AMT_INCOME_TOTAL\"] > 112500)&(df[\"AMT_INCOME_TOTAL\"] <= 225000) ,\"NEW_SEGMENT_INCOME\"] = \"Middle_Income\"\n","        df.loc[(df[\"AMT_INCOME_TOTAL\"] > 225000),\"NEW_SEGMENT_INCOME\"] = \"High_Income\"\n","\n","        # 15. Who was the person with when applying?\n","        df.loc[df['NAME_TYPE_SUITE'] == 'Unaccompanied', 'NEW_TYPE_SUITE_CAT'] = 0\n","        df.loc[df['NAME_TYPE_SUITE'] != 'Unaccompanied', 'NEW_TYPE_SUITE_CAT'] = 1\n","        df.loc[df['NAME_TYPE_SUITE'].isnull(), 'NEW_TYPE_SUITE_CAT'] = np.nan\n","\n","        df.loc[(df['DEF_30_CNT_SOCIAL_CIRCLE'] > 0) & (df['DEF_60_CNT_SOCIAL_CIRCLE'] > 0),\n","               'NEW_DEF_30_60_SOCIAL_CIRCLE'] = 1\n","        df.loc[(df['DEF_30_CNT_SOCIAL_CIRCLE'] > 0) & (df['DEF_60_CNT_SOCIAL_CIRCLE'] == 0),\n","               'NEW_DEF_30_60_SOCIAL_CIRCLE'] = 1\n","        df.loc[(df['DEF_30_CNT_SOCIAL_CIRCLE'] == 0) & (df['DEF_60_CNT_SOCIAL_CIRCLE'] > 0),\n","               'NEW_DEF_30_60_SOCIAL_CIRCLE'] = 1\n","        df.loc[(df['DEF_30_CNT_SOCIAL_CIRCLE'] == 0) & (df['DEF_60_CNT_SOCIAL_CIRCLE'] == 0),\n","               'NEW_DEF_30_60_SOCIAL_CIRCLE'] = 0\n","           ########################\n","        # One-Hot Encoding\n","        ########################\n","        df, cat_cols = self.one_hot_encoder(df, nan_as_category=False)\n","\n","        # Dropping feature named index\n","#         df.drop('index', axis=1, inplace=True)\n","\n","        gc.collect()\n","#         print(df.columns.tolist())\n","        return df\n","    \n","    \n","    def selected_var_via_gini(self, path_agg, time = 'full'):\n","        \"\"\"\n","        time - POS_CASH_balance : MONTHS_BALANCE\n","        \"\"\"\n","        if path_agg == 'installments_payments':\n","            agg_dat = self.installments_payments()\n","            agg_dat = agg_dat.reset_index()\n","            \n","        elif path_agg == 'previous_application':\n","            agg_dat = self.previous_application()\n","            agg_dat = agg_dat.reset_index()\n","            \n","        elif path_agg == 'credit_card_balance':\n","            agg_dat = self.credit_card_balance()\n","            agg_dat = agg_dat.reset_index()\n","            \n","        elif path_agg == 'POS_CASH_balance':\n","            agg_dat = self.agg_pos_cash_time(time = time)\n","            agg_dat = agg_dat.reset_index()\n","            \n","        elif path_agg in ['bureau', 'bureau_balance']:\n","            agg_dat = self.agg_bureau_and_balance() \n","            agg_dat = agg_dat.reset_index()\n","            \n","        elif path_agg == 'application_train':\n","            agg_dat = self.agg_application_train()\n","            agg_dat = agg_dat.reset_index()\n","            \n","        if 'index' in agg_dat.columns:\n","            agg_dat.drop(columns = ['index'])\n","            \n","            \n","        application_train = pd.read_csv(self.path_data['application_train'])[['SK_ID_CURR', 'TARGET']]\n","#         print('application_train = {}, agg_dat = {}'.format(application_train.shape,\n","#                                                             agg_dat.shape))\n","        merge_dat = pd.merge(application_train, agg_dat, on = ['SK_ID_CURR'], how = 'left')\n","#         print('dat merge = ', merge_dat.shape)\n","        \n","        if 'TARGET_x' in merge_dat.columns:\n","            merge_dat = merge_dat.drop(columns = ['TARGET_y'])\n","            merge_dat = merge_dat.rename(columns = {'TARGET_x': 'TARGET'})\n","            \n","        del application_train\n","        gc.collect()\n","\n","        newDF, woeDF = self.iv_woe(data = merge_dat.drop(columns = ['SK_ID_CURR']),\n","                            target = 'TARGET')\n","        del merge_dat\n","        gc.collect()\n","        agg_col = list(agg_dat.columns)\n","        dat = newDF[(newDF['IV']>= 0.02) & (newDF['Variable'].isin(agg_col))]\n","\n","        selected_cols = [col for col in dat['Variable']] + ['SK_ID_CURR']\n","        return dat, agg_dat[selected_cols]\n","    \n","    \n","    def merge_full_agg_table(self):\n","        print('application train ')\n","        _, agg_init = self.selected_var_via_gini(path_agg = 'application_train')\n","        print('done application train ')\n","        # test \n","#         _, agg_temp = self.selected_var_via_gini(path_agg = 'POS_CASH_balance')\n","#         print('done agg_temp')\n","        for path in tqdm(['POS_CASH_balance','installments_payments', 'previous_application', \n","                    'credit_card_balance', 'bureau']): # installments_payments\n","            try:\n","                _, agg_dat = self.selected_var_via_gini(path_agg = path)\n","                agg_init = pd.merge(agg_init, agg_dat, on = 'SK_ID_CURR', how = 'left')\n","                print('pass ', path)\n","            except:\n","                print('FAIL ', path)\n","            \n","        application_train = pd.read_csv(self.path_data['application_train'])[['SK_ID_CURR', 'TARGET']]\n","        agg_init = pd.merge(agg_init, application_train, on = ['SK_ID_CURR'], how = 'left')\n","        \n","        return agg_init\n","        \n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"4699cd0b","metadata":{"execution":{"iopub.execute_input":"2023-03-28T10:26:31.078970Z","iopub.status.busy":"2023-03-28T10:26:31.077846Z","iopub.status.idle":"2023-03-28T10:26:31.087091Z","shell.execute_reply":"2023-03-28T10:26:31.085763Z"},"papermill":{"duration":0.019233,"end_time":"2023-03-28T10:26:31.089508","exception":false,"start_time":"2023-03-28T10:26:31.070275","status":"completed"},"tags":[],"id":"4699cd0b"},"outputs":[],"source":["agg = AGG_DATA(path_data = {\n","    'application_train': '/content/drive/MyDrive/dseb-neu/Senior/HomeCredit/DATA/application_train.csv/application_train.csv',\n","    'installments_payments': '/content/drive/MyDrive/dseb-neu/Senior/HomeCredit/DATA/installments_payments.csv/installments_payments.csv',\n","    'previous_application': '/content/drive/MyDrive/dseb-neu/Senior/HomeCredit/DATA/previous_application.csv/previous_application.csv',\n","    'credit_card_balance' : '/content/drive/MyDrive/dseb-neu/Senior/HomeCredit/DATA/credit_card_balance.csv/credit_card_balance.csv',\n","    'POS_CASH_balance': '/content/drive/MyDrive/dseb-neu/Senior/HomeCredit/DATA/POS_CASH_balance/POS_CASH_balance.csv/POS_CASH_balance.csv',\n","    'bureau': '/content/drive/MyDrive/dseb-neu/Senior/HomeCredit/DATA/bureau.csv/bureau.csv',\n","    'bureau_balance': '/content/drive/MyDrive/dseb-neu/Senior/HomeCredit/DATA/bureau_balance.csv/bureau_balance.csv'\n","})"]},{"cell_type":"code","execution_count":null,"id":"f5ca6f99","metadata":{"execution":{"iopub.execute_input":"2023-03-28T10:26:31.106345Z","iopub.status.busy":"2023-03-28T10:26:31.105580Z","iopub.status.idle":"2023-03-28T10:31:39.019320Z","shell.execute_reply":"2023-03-28T10:31:39.017020Z"},"papermill":{"duration":307.92751,"end_time":"2023-03-28T10:31:39.023326","exception":false,"start_time":"2023-03-28T10:26:31.095816","status":"completed"},"tags":[],"id":"f5ca6f99","outputId":"34f71a14-aa95-4a74-f217-e251e38b8989"},"outputs":[{"name":"stdout","output_type":"stream","text":["application train \n","done application train \n"]},{"name":"stderr","output_type":"stream","text":[" 20%|██        | 1/5 [00:29<01:57, 29.46s/it]"]},{"name":"stdout","output_type":"stream","text":["pass  POS_CASH_balance\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|████      | 2/5 [01:27<02:18, 46.15s/it]"]},{"name":"stdout","output_type":"stream","text":["pass  installments_payments\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:231: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"," 60%|██████    | 3/5 [02:58<02:13, 66.85s/it]"]},{"name":"stdout","output_type":"stream","text":["pass  previous_application\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|████████  | 4/5 [03:39<00:56, 56.61s/it]"]},{"name":"stdout","output_type":"stream","text":["pass  credit_card_balance\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 5/5 [04:33<00:00, 54.76s/it]"]},{"name":"stdout","output_type":"stream","text":["pass  bureau\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["(307505, 363)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>CODE_GENDER</th>\n","      <th>AMT_CREDIT</th>\n","      <th>AMT_ANNUITY</th>\n","      <th>AMT_GOODS_PRICE</th>\n","      <th>REGION_POPULATION_RELATIVE</th>\n","      <th>DAYS_BIRTH</th>\n","      <th>DAYS_EMPLOYED</th>\n","      <th>DAYS_REGISTRATION</th>\n","      <th>DAYS_ID_PUBLISH</th>\n","      <th>OWN_CAR_AGE</th>\n","      <th>...</th>\n","      <th>CLOSED_AMT_CREDIT_SUM_DEBT_SUM</th>\n","      <th>CLOSED_AMT_CREDIT_SUM_OVERDUE_MEAN</th>\n","      <th>CLOSED_AMT_CREDIT_SUM_LIMIT_MEAN</th>\n","      <th>CLOSED_AMT_CREDIT_SUM_LIMIT_SUM</th>\n","      <th>CLOSED_CNT_CREDIT_PROLONG_SUM</th>\n","      <th>CLOSED_MONTHS_BALANCE_SIZE_SUM</th>\n","      <th>CLOSED_NEW_DEPT_RATIO_MIN</th>\n","      <th>CLOSED_NEW_DEPT_RATIO_MAX</th>\n","      <th>CLOSED_NEW_DEPT_RATIO_MEAN</th>\n","      <th>TARGET</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>406597.5</td>\n","      <td>24700.5</td>\n","      <td>351000.0</td>\n","      <td>0.018801</td>\n","      <td>-9461</td>\n","      <td>-637.0</td>\n","      <td>-3648.0</td>\n","      <td>-2120</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>90.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1293502.5</td>\n","      <td>35698.5</td>\n","      <td>1129500.0</td>\n","      <td>0.003541</td>\n","      <td>-16765</td>\n","      <td>-1188.0</td>\n","      <td>-1186.0</td>\n","      <td>-291</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>135000.0</td>\n","      <td>6750.0</td>\n","      <td>135000.0</td>\n","      <td>0.010032</td>\n","      <td>-19046</td>\n","      <td>-225.0</td>\n","      <td>-4260.0</td>\n","      <td>-2531</td>\n","      <td>26.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>312682.5</td>\n","      <td>29686.5</td>\n","      <td>297000.0</td>\n","      <td>0.008019</td>\n","      <td>-19005</td>\n","      <td>-3039.0</td>\n","      <td>-9833.0</td>\n","      <td>-2437</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>513000.0</td>\n","      <td>21865.5</td>\n","      <td>513000.0</td>\n","      <td>0.028663</td>\n","      <td>-19932</td>\n","      <td>-3038.0</td>\n","      <td>-4311.0</td>\n","      <td>-3458</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 363 columns</p>\n","</div>"],"text/plain":["   CODE_GENDER  AMT_CREDIT  AMT_ANNUITY  AMT_GOODS_PRICE  \\\n","0            0    406597.5      24700.5         351000.0   \n","1            1   1293502.5      35698.5        1129500.0   \n","2            0    135000.0       6750.0         135000.0   \n","3            1    312682.5      29686.5         297000.0   \n","4            0    513000.0      21865.5         513000.0   \n","\n","   REGION_POPULATION_RELATIVE  DAYS_BIRTH  DAYS_EMPLOYED  DAYS_REGISTRATION  \\\n","0                    0.018801       -9461         -637.0            -3648.0   \n","1                    0.003541      -16765        -1188.0            -1186.0   \n","2                    0.010032      -19046         -225.0            -4260.0   \n","3                    0.008019      -19005        -3039.0            -9833.0   \n","4                    0.028663      -19932        -3038.0            -4311.0   \n","\n","   DAYS_ID_PUBLISH  OWN_CAR_AGE  ...  CLOSED_AMT_CREDIT_SUM_DEBT_SUM  \\\n","0            -2120          NaN  ...                             0.0   \n","1             -291          NaN  ...                             0.0   \n","2            -2531         26.0  ...                             0.0   \n","3            -2437          NaN  ...                             NaN   \n","4            -3458          NaN  ...                             0.0   \n","\n","   CLOSED_AMT_CREDIT_SUM_OVERDUE_MEAN  CLOSED_AMT_CREDIT_SUM_LIMIT_MEAN  \\\n","0                                 0.0                               0.0   \n","1                                 0.0                               0.0   \n","2                                 0.0                               0.0   \n","3                                 NaN                               NaN   \n","4                                 0.0                               0.0   \n","\n","   CLOSED_AMT_CREDIT_SUM_LIMIT_SUM  CLOSED_CNT_CREDIT_PROLONG_SUM  \\\n","0                              0.0                            0.0   \n","1                              0.0                            0.0   \n","2                              0.0                            0.0   \n","3                              NaN                            NaN   \n","4                              0.0                            0.0   \n","\n","   CLOSED_MONTHS_BALANCE_SIZE_SUM  CLOSED_NEW_DEPT_RATIO_MIN  \\\n","0                            90.0                        0.0   \n","1                             0.0                        0.0   \n","2                             0.0                        0.0   \n","3                             NaN                        NaN   \n","4                             0.0                        0.0   \n","\n","   CLOSED_NEW_DEPT_RATIO_MAX  CLOSED_NEW_DEPT_RATIO_MEAN  TARGET  \n","0                        0.0                         0.0       1  \n","1                        0.0                         0.0       0  \n","2                        0.0                         0.0       0  \n","3                        NaN                         NaN       0  \n","4                        0.0                         0.0       0  \n","\n","[5 rows x 363 columns]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["data = agg.merge_full_agg_table()\n","print(data.shape)\n","data.head()"]},{"cell_type":"code","execution_count":null,"id":"267d2969","metadata":{"execution":{"iopub.execute_input":"2023-03-28T10:31:39.043453Z","iopub.status.busy":"2023-03-28T10:31:39.042641Z","iopub.status.idle":"2023-03-28T10:31:39.927270Z","shell.execute_reply":"2023-03-28T10:31:39.926159Z"},"papermill":{"duration":0.897774,"end_time":"2023-03-28T10:31:39.930465","exception":false,"start_time":"2023-03-28T10:31:39.032691","status":"completed"},"tags":[],"id":"267d2969"},"outputs":[],"source":["import copy \n","data_cop = copy.deepcopy(data)"]},{"cell_type":"code","execution_count":null,"id":"7a06838a","metadata":{"execution":{"iopub.execute_input":"2023-03-28T10:31:39.950147Z","iopub.status.busy":"2023-03-28T10:31:39.949137Z","iopub.status.idle":"2023-03-28T10:31:41.220196Z","shell.execute_reply":"2023-03-28T10:31:41.219148Z"},"papermill":{"duration":1.28273,"end_time":"2023-03-28T10:31:41.223051","exception":false,"start_time":"2023-03-28T10:31:39.940321","status":"completed"},"tags":[],"id":"7a06838a"},"outputs":[],"source":["data.to_pickle('/content/drive/MyDrive/dseb-neu/Senior/HomeCredit/DATA/agg_data (1).pkl')"]},{"cell_type":"code","execution_count":null,"id":"35aaf27e","metadata":{"papermill":{"duration":0.006958,"end_time":"2023-03-28T10:31:41.237383","exception":false,"start_time":"2023-03-28T10:31:41.230425","status":"completed"},"tags":[],"id":"35aaf27e"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"47ecf0c8","metadata":{"papermill":{"duration":0.006805,"end_time":"2023-03-28T10:31:41.251386","exception":false,"start_time":"2023-03-28T10:31:41.244581","status":"completed"},"tags":[],"id":"47ecf0c8"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"72405c85","metadata":{"papermill":{"duration":0.006833,"end_time":"2023-03-28T10:31:41.265343","exception":false,"start_time":"2023-03-28T10:31:41.258510","status":"completed"},"tags":[],"id":"72405c85"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"17113ed1","metadata":{"papermill":{"duration":0.006741,"end_time":"2023-03-28T10:31:41.279108","exception":false,"start_time":"2023-03-28T10:31:41.272367","status":"completed"},"tags":[],"id":"17113ed1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"bebb031c","metadata":{"papermill":{"duration":0.006716,"end_time":"2023-03-28T10:31:41.292877","exception":false,"start_time":"2023-03-28T10:31:41.286161","status":"completed"},"tags":[],"id":"bebb031c"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"e1cb5533","metadata":{"papermill":{"duration":0.006745,"end_time":"2023-03-28T10:31:41.306571","exception":false,"start_time":"2023-03-28T10:31:41.299826","status":"completed"},"tags":[],"id":"e1cb5533"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"7db99716","metadata":{"papermill":{"duration":0.006686,"end_time":"2023-03-28T10:31:41.320292","exception":false,"start_time":"2023-03-28T10:31:41.313606","status":"completed"},"tags":[],"id":"7db99716"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"fd19452d","metadata":{"papermill":{"duration":0.006645,"end_time":"2023-03-28T10:31:41.333915","exception":false,"start_time":"2023-03-28T10:31:41.327270","status":"completed"},"tags":[],"id":"fd19452d"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"5cd421b9","metadata":{"papermill":{"duration":0.006582,"end_time":"2023-03-28T10:31:41.347375","exception":false,"start_time":"2023-03-28T10:31:41.340793","status":"completed"},"tags":[],"id":"5cd421b9"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"c09357f0","metadata":{"papermill":{"duration":0.006568,"end_time":"2023-03-28T10:31:41.360821","exception":false,"start_time":"2023-03-28T10:31:41.354253","status":"completed"},"tags":[],"id":"c09357f0"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"0cfc82cb","metadata":{"papermill":{"duration":0.006582,"end_time":"2023-03-28T10:31:41.374341","exception":false,"start_time":"2023-03-28T10:31:41.367759","status":"completed"},"tags":[],"id":"0cfc82cb"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"e9d0459c","metadata":{"papermill":{"duration":0.006874,"end_time":"2023-03-28T10:31:41.388163","exception":false,"start_time":"2023-03-28T10:31:41.381289","status":"completed"},"tags":[],"id":"e9d0459c"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":321.141805,"end_time":"2023-03-28T10:31:42.117500","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-03-28T10:26:20.975695","version":"2.4.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}